# Spark + Hudi + Iceberg + Airflow + MinIO
Проект представляет собой ETL-платформу для обработки данных с использованием Apache Spark, Apache Hudi, Apache Iceberg, Apache Airflow и MinIO (S3-совместимое объектное хранилище).
## Основные возможности
- **Многослойная архитектура данных**: Bronze → Silver → Gold
- **Управление данными**: Apache Hudi для ACID-транзакций и upsert операций
- **Оркестрация**: Apache Airflow для планирования и мониторинга ETL-процессов
- **Хранилище**: MinIO как S3-совместимое объектное хранилище
- **Мониторинг**: Spark History Server и Airflow UI
## Архитектура
Bronze Layer (Raw) → Silver Layer (Cleaned) → Gold Layer (Aggregated)
## Предварительные требования
- Docker
- Docker Compose
- 8GB+ свободной оперативной памяти
- 10GB+ свободного места на диске
## Быстрый старт
### Клонирование и настройка
```bash
mkdir -p shared_data/hudi_dag minio/data minio/config spark/jars
```
##### Исходные данные (.csv) в minio/data/bronze-layer
##### customers.csv и articles.csv должны быть в соответствующих папках
### Запуск всей платформы
```bash
docker-compose up -d
```
Доступ к сервисам
После запуска будут доступны следующие интерфейсы:
| Сервис | URL | Порт | Назначение |
|--------|-----|------|------------|
| Airflow UI | http://localhost:8080 | 8080 | Оркестрация DAGs |
| Jupyter Notebook | http://localhost:8888 | 8888 | Интерактивная разработка |
| Spark Master UI | http://localhost:8081 | 8081 | Мониторинг Spark |
| Spark History Server | http://localhost:18080 | 18080 | История Spark jobs |
| MinIO Console | http://localhost:9001 | 9001 | Управление объектным хранилищем |
## Добавление новых DAGs
- Разместите Python файл с описанием DAG в dags/ директории, все остальные исходники в shared_data/
- Убедитесь что зависимости установлены в Airflow контейнере
- Перезапустите Airflow scheduler и webserver
## Описание примера DAG с Hudi
##### `bronze_to_silver.py` - Bronze → Silver слой
| Компонент | Функциональность | Особенности |
|-----------|------------------|-------------|
| **Spark Session** | Создание сессии с Hudi | Adaptive Query Execution, Hudi расширения |
| **Обработка клиентов** | Дедупликация, обогащение, типизация | Возрастные категории, хеширование, партиционирование по времени |
| **Обработка товаров** | Очистка, оценка качества, дедупликация | Скоринг полноты данных (22 поля), партиционирование по категориям |
| **Hudi таблицы** | COPY_ON_WRITE (клиенты), MERGE_ON_READ (товары) | Bloom индексы, автоматическая очистка, кластеризация |
##### `silver_to_gold.py` - Silver → Gold слой
| Компонент | Аналитические метрики | Технические особенности |
|-----------|----------------------|-------------------------|
| **Аналитика клиентов** | Статистика по возрастам, распределение по категориям | Квартили, IQR, медианные значения |
| **Аналитика товаров** | Метрики разнообразия, оценка качества | Индексы сложности, уникальность атрибутов |
| **Кросс-аналитика** | Сводные метрики, потенциальные комбинации | Data Quality Index, агрегация по всей системе |
| **Hudi оптимизации** | Глобальные Bloom индексы, кэширование | Различные стратегии партиционирования |


## Описание DAG с Iceberg для данных о зарплатах
### `iceberg_salary_dag` - ETL pipeline для анализа зарплат
#### Архитектура DAG
create_buckets → bronze_to_silver → silver_to_gold
#### Компоненты DAG
##### Инициализация
| Компонент | Назначение | Конфигурация |
|-----------|------------|--------------|
| **MinIO Buckets** | Создание необходимых бакетов | `iceberg-warehouse`, `bronze-layer` |
| **Iceberg Catalog** | Инициализация каталога | Hadoop FileIO, S3A для MinIO |

##### `bronze_to_silver.py` - Обработка сырых данных о зарплатах
| Этап обработки | Функциональность | Особенности |
|---------------|------------------|-------------|
| **Чтение данных** | CSV из MinIO Bronze Layer | Автоматическое создание тестовых данных при отсутствии |
| **Очистка и стандартизация** | Валидация и нормализация полей | Стандартизация образования, должностей, гендера |
| **Обогащение данных** | Расчет производных полей | Уровень опыта, возрастные группы, категории зарплат |
| **Дедупликация** | Удаление дубликатов | По ключевым атрибутам (возраст, гендер, образование, должность, опыт) |
| **Запись в Iceberg** | Silver слой таблиц | Партиционирование по году/месяцу |
**Таблицы Silver слоя:**
- `salary_detailed` - детальные данные с партиционированием
- `salary_education_stats` - статистика по образованию  
- `salary_job_stats` - статистика по должностям
- `salary_gender_stats` - статистика по гендеру
##### `silver_to_gold.py` - Аналитика и агрегации
| Аналитический модуль | Метрики | Бизнес-ценность |
|---------------------|---------|-----------------|
| **Тренды по опыту** | Квартили, стандартное отклонение, диапазон | Анализ роста зарплат с опытом |
| **Влияние образования** | Премия за образование, максимальные зарплаты | ROI образования |
| **Гендерный анализ** | Медианные зарплаты, распределение по образованию | Анализ pay gap |
| **Анализ должностей** | Рейтинг позиций, разнообразие требований | Топ высокооплачиваемых позиций |
| **Индустриальные бенчмарки** | Отраслевые метрики, качество данных | Market positioning |
#### Технические особенности Iceberg
##### Конфигурация
```python
# Каталог и FileIO
"spark.sql.catalog.spark_catalog.type": "hadoop"
"spark.sql.catalog.spark_catalog.io-impl": "org.apache.iceberg.hadoop.HadoopFileIO"

# MinIO S3A конфигурация
"spark.hadoop.fs.s3a.endpoint": "http://minio:9000"
"spark.hadoop.fs.s3a.path.style.access": "true"
```
**Исходные поля (Bronze):**
```text
Age, Gender, Education Level, Job Title, Years of Experience, Salary
```
#### Обработанные поля (Silver/Gold):
- **Базовые атрибуты**: age, gender, education_level, job_title, years_of_experience, salary
- **Производные категории**: experience_level, age_group, salary_bucket
- **Временные метки**: update_timestamp, etl_date, year, month
- **Хеши для дедупликации**: data_hash